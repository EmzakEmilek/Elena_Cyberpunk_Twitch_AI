#!/usr/bin/env bash
# ğŸš€ ONE-CLICK QUEST GENERATOR SETUP
# Complete automated setup for Cyberpunk Quest Generator

echo "ğŸš€ CYBERPUNK QUEST GENERATOR - ONE-CLICK SETUP"
echo "=============================================="

# Function to check command exists
command_exists() {
    command -v "$1" >/dev/null 2>&1
}

# Function to detect OS
detect_os() {
    if [[ "$OSTYPE" == "msys" || "$OSTYPE" == "cygwin" || "$OSTYPE" == "win32" ]]; then
        echo "windows"
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        echo "macos"
    else
        echo "linux"
    fi
}

OS=$(detect_os)
echo "ğŸ–¥ï¸ Detected OS: $OS"

# Check Python
if ! command_exists python3; then
    echo "âŒ Python 3 not found. Please install Python 3.8+"
    exit 1
fi

PYTHON_VERSION=$(python3 -c 'import sys; print(".".join(map(str, sys.version_info[:2])))')
echo "ğŸ Python version: $PYTHON_VERSION"

# Check GPU (NVIDIA)
if command_exists nvidia-smi; then
    echo "ğŸ® NVIDIA GPU detected:"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
else
    echo "âš ï¸ No NVIDIA GPU detected - CPU mode only"
fi

# Install Python dependencies
echo "ğŸ“¦ Installing Python dependencies..."
python3 -m pip install requests PyYAML beautifulsoup4 tqdm colorlog aiofiles

# Install Ollama based on OS
echo "ğŸ”½ Installing Ollama..."

if [[ "$OS" == "linux" ]]; then
    curl -fsSL https://ollama.ai/install.sh | sh
elif [[ "$OS" == "macos" ]]; then
    if command_exists brew; then
        brew install ollama
    else
        echo "ğŸ“¥ Please download Ollama from: https://ollama.ai/download/mac"
        read -p "Press Enter after installing Ollama..."
    fi
elif [[ "$OS" == "windows" ]]; then
    echo "ğŸ“¥ Please download and install Ollama from: https://ollama.ai/download/windows"
    read -p "Press Enter after installing Ollama..."
fi

# Start Ollama server
echo "ğŸš€ Starting Ollama server..."
ollama serve &
OLLAMA_PID=$!
sleep 5

# Download model
echo "ğŸ¤– Available models:"
echo "1. llama3.1:8b (Recommended - 8GB VRAM)"
echo "2. llama3.1:70b (Best quality - 40GB+ VRAM)"
echo "3. qwen2.5:7b (Alternative - 8GB VRAM)"
echo "4. Skip model download"

read -p "Choose model (1-4): " model_choice

case $model_choice in
    1)
        echo "ğŸ“¥ Downloading llama3.1:8b..."
        ollama pull llama3.1:8b
        ;;
    2)
        echo "ğŸ“¥ Downloading llama3.1:70b..."
        ollama pull llama3.1:70b
        ;;
    3)
        echo "ğŸ“¥ Downloading qwen2.5:7b..."
        ollama pull qwen2.5:7b
        ;;
    4)
        echo "â­ï¸ Skipping model download"
        ;;
    *)
        echo "â­ï¸ Invalid choice, skipping model download"
        ;;
esac

# Create quest generator structure
echo "ğŸ—ï¸ Setting up quest generator..."
mkdir -p quest_generator/{data_sources,llm_engine,generators,output}

# Test Ollama connection
echo "ğŸ” Testing Ollama connection..."
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "âœ… Ollama is running and accessible"
else
    echo "âŒ Ollama not accessible. Please start manually: ollama serve"
    exit 1
fi

# Show available models
echo "ğŸ“‹ Available models:"
ollama list

# Final instructions
echo ""
echo "ğŸ‰ SETUP COMPLETED!"
echo "=================="
echo ""
echo "ğŸ“‹ NEXT STEPS:"
echo "1. Run quest generator:"
echo "   python3 quest_generator/generate_quests.py"
echo ""
echo "2. Generated quests will be saved to:"
echo "   lore/quests/main_story/"
echo "   lore/quests/side_quests/"
echo "   lore/quests/romance_quests/"
echo ""
echo "ğŸ’¡ TIPS:"
echo "â€¢ Generation takes ~2-3 hours for all quests"
echo "â€¢ Monitor GPU/CPU usage during generation"
echo "â€¢ Backup existing quests before running"
echo ""
echo "ğŸ¤– MODELS USAGE:"
echo "â€¢ Edit model name in generate_quests.py"
echo "â€¢ Default: llama3.1:8b"
echo "â€¢ For better quality: llama3.1:70b"
echo ""
echo "ğŸš€ START GENERATION:"
echo "python3 quest_generator/generate_quests.py"
